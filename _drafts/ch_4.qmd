---
title: "Chapter 4"
subtitle: "Using sensing data to predict opioid lapse risk in a national sample of patients with opioid use disorder"
format: html
date: last-modified
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: false

library(tidyverse) |> 
  suppressMessages()

options(knitr.kable.NA = '')
```


# Introduction

Studies show high agreement between recent (i.e., 1-4 days) self-report and biological markers (i.e., urine, saliva, hair) of drug use [@bharatAgreementSelfreportedIllicit2023]. This suggests people may be willing to report illicit substance use behaviors. 

However, it is unclear if people in recovery from substance use disorders, other than alcohol, can sustain long-term adherence (e.g., one year or more) needed for a recovery monitoring support system that uses self-report data. Previous studies examining adherence to frequent self-report prompts among people with illicit substance use disorders have typically only prompted for 7-30 days [@jonesComplianceEcologicalMomentary2019] (except @kennedySexDifferencesCocaine2013 prompted for 175 days and found 75% adherence).

It is also unclear whether people in recovery from illicit substance use disorders can, or are willing to, accurately report other risk information needed to develop accurate prediction models. For example, people with substance use disorders may experience greater instability in their day-to-day lives (e.g., stigma or legal consequences may make access to healthcare, stable housing, or supportive relationships more difficult). This instability could make it difficult to recall and report recent behaviors and events promptly or accurately. It may also skew their baseline perception of what constitutes a risky or stressful experience. 

Supplementing self-report data with passively sensed data (e.g., geolocation) could make up for imprecise reports of risk factors or be used for lapse prediction during periods of non-adherence to self-report surveys. Additionally, more data will produce more features that could allow for better personalization of support recommendations.

This project will use daily surveys and sensed geolocation for up to one year from a national sample of people with opioid use disorder to predict immediate (i.e., in the next 24 hours) lapses back to opioid use. 

# Specific Aims

In this study, we will expand our previous modeling procedure to opioid use disorder and evaluate performance, fairness, and top features for predicting opioid lapse risk.

Specifically, we will:

**1. Evaluate the performance (auROC) of a machine learning model that predicts opioid lapses from geolocation and daily surveys.** This aim will allow us to determine whether lapse prediction models can be generalized to other drugs beyond alcohol. Notably, a successful model will demonstrate that lapse prediction can be done with a drug where its use is illegal and people may be less willing to provide information about lapses and risk factors. It will also show the feasibility of using self-report data over long periods of reocvery (i.e., 12 months).

**2. Assess model fairness in performance across important subgroups with known disparities in substance use treatment access and/or outcomes - race/ethnicity (not White vs. non-Hispanic White), income (below \$25,000 vs. above \$25,000), gender (female vs. male vs. other), and geographic location (rural vs. urban).** These data offer more diversity with regard to race/ethnicity, income, and geographic location. This aim will allow us to determine if improving the quality of the training data with respect to diversity is sufficient to address issues of fairness.  

**3. Describe the relative importance of features on model performance.** Model features will be derived from two complementary sensing methods: daily surveys and continuous geolocation data. Geolocation sensing, a passive sensing method, could compliment daily surveys well. It could provide insight into information difficult to measure with self-report (e.g., the amount of time spent in risky locations, or changes in routine that could indicate life stressors) or that would add additional burden by increasing the number of questions on the daily surveys. Furthermore, by adding more data sources gives us more features and that could mean better personalization of predictions and recommendations for more people. This aim will help determine whether a sufficient number of unique important features emerge from these data.

# Methods

## Participants
We recruited participants in early recovery from opioid use disorder. Participants were recruited through print and targeted digital advertisements (craigslist, reddit, Facebook) and partnerships with MOUD treatment centers. We required that participants:  

1. were age 18 or older,
2. could write, speak, and read in English,
3. enrolled in an MOUD treatment program (for at least one month but not longer than 12 months) and adherent (taken daily medication every day or nearly every day in past month) or enrolled in or recently completed an intensive outpatient treatment program for opioid use disorder,
4. had a goal of abstinence from opioids,
5. had an android smartphone that they were willing to use as their single phone for duration of the study, and
6. had active cellular plan that they were willing to maintain for duration of the study.

Participants were considered enrolled in the study if they were eligible, consented, and provided data for at least one month. A total of 336 participants enrolled in the study. We excluded data from one participant whose geolocation data showed they did not reside in the US. We also excluded 11 participants due to careless responding on the daily surveys and/or lapses reported nearly every day on study, suggesting they did not have a goal of abstinence. Our final sample consisted of 324 participants.

The table below presents the demographic and clinical characteristics of the 324 participants in our analysis sample.

```{r}
#| echo: false
#| message: false

dem <- read_csv(here::here("data/risk2_dem.csv"),
                show_col_types = FALSE) |> 
  rename(` ` = `...1`)

dem |> 
  knitr::kable(digits = 1) |> 
  kableExtra::kable_classic() |> 
  kableExtra::group_rows(start_row = 2, end_row = 8) |> 
  kableExtra::group_rows(start_row = 10, end_row = 14) |> 
  kableExtra::group_rows(start_row = 16, end_row = 20) |> 
  kableExtra::group_rows(start_row = 22, end_row = 28) |> 
  kableExtra::group_rows(start_row = 30, end_row = 35) |> 
  kableExtra::group_rows(start_row = 37, end_row = 42) |> 
  kableExtra::group_rows(start_row = 44, end_row = 51) |> 
  kableExtra::group_rows(start_row = 53, end_row = 54) |> 
  kableExtra::group_rows(start_row = 57, end_row = 60) |> 
  kableExtra::group_rows(start_row = 62, end_row = 66) |> 
  kableExtra::group_rows(start_row = 68, end_row = 69)
```

## Procedure
Participants who screened eligible were consented and onboarded over video or phone call. All participants were instructed to download the STARR study app (a version of CHESS). After this meeting participants were instructed to watch a set of video tutorials for learning how to use the app. One week later they participated in a check-in video or phone call with study staff to answer any questions about the app and troubleshoot any technical issues. At this time, study staff also mailed onboarding materials to participants, including a payment card. Participants were expected to complete monthly surveys to remain on study. At the end of the study participants met with study staff for a final debriefing video or phone call.

## Measures

### Individual Characteristics
We collected self-report information about demographics (age, gender, orientation, race/ethnicity, education, employment, income, relationship status, location) and clinical characteristics (DSM-5 OUD symptom count, MOUD medication, number of lifetime overdoses) to characterize our sample. Demographic information will be included as features in our models. A subset of these variables (gender, race/ethnicity, income, and location) will be used for model fairness analyses, as they have documented disparities in treatment access and outcomes.

As part of the aims of the parent project we collected many other trait and state measures throughout the study. A complete list of all measures will be made available on our study’s OSF page.

### Daily Surveys
Participants completed one brief (16 questions) daily survey. Daily surveys became available in star app each morning at 6 am. The survey remind open until 5:59 am the next day. Push notifications were also sent to participants to remind them that they had a new task that had not been completed yet.

On each survey, participants reported dates/times of any previously unreported past opioid use. They also reported any other drugs or alcohol used in the past 24 hours and whether they took their MOUD as prescribed. Next, participants rated the maximum intensity of recent (i.e., since last EMA) experiences of pain, craving, risky situations, stressful events, and pleasant events. Next, participants rated their sleep and how depressed, angry, anxious, relaxed, and happy they have felt in the past 24 hours. Finally, participants rated how motivated they were to completely avoid using opioids for non-medical reasons and how confident they were in their ability to completely avoid using opioids for non-medical reasons.

Participants were withdrawn early from the study if they did not complete at least 20 daily surveys in a four week period.

### Monthly Surveys
Monthly surveys consisted of several clinical scales as part of the parent project's aims. The monthly survey was also personalized to ask a series of questions about participants' frequent locations (identified by geolocation sensing - see Sensed Geolocation section below). For locations that participants visited twice in a month, they were asked to identify the type of location, what they do there, how pleasant and unpleasant their experience is there, and how much this place helps and harms their recovery from opioids. 

Participants were withdrawn early from the study if they missed three monthly surveys.

### Sensed Geolocation
Continuous sensed geolocation was collected through the STARR app. Geolocation was contextualized by asking questions about frequently visited locations in each monthly survey (see Monthly Surveys section above). 

Participants were shown how to temporarily turn off sharing geolocation with us. However, participants were expected to share their location with the STARR app and were withdrawn from the study if they did consistently provide geolocation data (i.e., disabling location sharing for more than 12 hours in a four-week period). 

## Planned Data Analyses

### Labels
Prediction windows are 24 hours in width. The 24-hour windows roll day-by-day starting at 6 am in the participant's own time zone. The start and end date/time of past opioid use were reported on the first daily survey item. Each prediction window was labeled as a lapse if opioid use was reported as occurring between 6 am that day and 5:59 am the next morning. Windows with no reported opioid use were labeled as no lapse. 

We ended up with a total of 93376 labels, with 2% labeled as lapses.

### Feature Engineering

Features will be derived from four sources:

1. Prediction window: We will dummy-code features for day of the week for the start of the prediction window.

2. Demographics: We will create dummy-coded features for age (18-25 years, 26-35 years, 36-45 years, 46-55 years, 56 years or older), personal income (less than \$25,000, more than \$25,000), gender (male, female, other), race/ethnicity (non-Hispanic White vs. not White), geographic location (urban, rural), relationship status (in committed relationship, not in committed relationsip), education (high school or less, some college, college degree), and employment (employed, not employed).

3. Previous daily survey responses: We will create raw and change features using daily surveys in varying feature scoring epochs (i.e., 48, 72, and 168 hours) before the start of the prediction window for all daily survey items. Raw features will include min, max, and median scores for each daily survey item across all daily surveys in each epoch for that participant. We will also calculate change features by subtracting each participant’s baseline mean score for each daily survey item from their raw feature. These baseline mean scores will be calculated using all of their daily surveys collected from the start of their participation until the start of the prediction window. We also will create raw and change features based on the most recent response for each daily survey question and raw and change rate features from previously reported lapses and number of completed daily surveys.

4. Geolocation data: We will calculate raw and change features for time spent in locations harmful to recovery, time spent in locations helpful for recovery, time spent in pleasant locations, time spent in unpleasant locations,  location variance, type of location, and activity done at location over varying feature scoring epochs (i.e., 6, 12, 24, 48, 72, and 168 hours).

Other generic feature engineering steps will include imputing missing data (median imputation for numeric features, mode imputation for nominal features) and removing zero and near-zero variance features as determined from held-in data (see Cross-validation section below).

### Model Training and Evaluation

#### Cross Validation
We will consider candidate Xgboost model configurations that differ across sensible values for key hyperparameters and outcome resampling method (i.e., no resampling and up-sampling and down-sampling of the outcome using majority/no lapse to minority/lapse ratios ranging from 1:1 to 5:1).

We will use participant-grouped, nested cross-validation for model training, selection, and evaluation with auROC. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (lapse) relative to a randomly selected negative case (no lapse). Grouped cross-validation assigns all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participant’s data from their own data. We will use 1 repeat of 10-fold cross-validation for the inner loops (i.e., validation sets) and 3 repeats of 10-fold cross-validation for the outer loop (i.e., test sets). Best model configurations will be selected using median auROC across the 10 validation sets. Final performance of these best model configurations will be evaluated using median auROC across the 30 test sets.

### Bayesian Model
We will use a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) from the 30 held-out test sets for our best model. We will use the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid over-fitting. We will set two random intercepts to account for our resampling method: one for the repeat, and another for the fold nested within repeat. 

From the Bayesian model we will obtain the posterior distribution and Bayeisan CI for auROCs our best model. To evaluate our models’ overall performance we will report the median posterior probability for auROC and Bayesian CI. This represents our best estimate for the magnitude of the auROC parameter. If the credible interval does not contain .5 (chance performance), this provides strong evidence (> .95 probability) that our model is capturing signal in the data.

### Fairness Analyses
We will calculate the median posterior probability and 95% Bayesian CI for auROC for our best model separately by race/ethnicity (not White vs. non-Hispanic White), income (below \$25,000 vs. above \$25,000), gender (female vs. male vs. other), and location (rural vs. urban). We will conduct Bayesian group comparisons to assess the likelihood that each model performs differently by group. We will report the precise posterior probability for the difference in auROCs and the 95% Bayesian CIs for each model comparison.

### Feature Importance
We will calculate Shapley values in log-odds units for binary classification models from the 30 test sets to provide a description of the importance of categories of features across our best model. We will average the three Shapley values for each observation for each feature (i.e., across the three repeats) to increase their stability. An inherent property of Shapley values is their additivity, allowing us to combine features into feature categories. We will create separate feature categories for each of the 15 daily survey questions, past opioid use, missing daily surveys, time spent in risky locations, and time spent at known locations (separate by type of location). We will calculate the local (i.e., for each observation) importance for each category of features by adding Shapley values across all features in a category, separately for each observation. We will calculate global importance for each feature category by averaging the absolute value of the Shapley values of all features in the category across all observations. These local and global importance scores based on Shapley values allow us to contextualize relative feature importance for our model.

